training_data <- read.table("~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/training_data.txt", header=T, quote="\"")
View(training_data)
test_data <- read.table("~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/test_data.txt", header=T, quote="\"")
View(test_data)
save.image("~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/training_data_and_test_data.RData")
library("gtools")
library("hash")
library("R.utils")
library("rbenchmark")
library("testthat")
sourceDir <- "~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/functions"
sourceDirectory(sourceDir)
# Data and parameters for MCMC
mObs <- as.matrix(training_data)
varNames <- names(training_data)
numNodes <- length(varNames)
cardinalities <- rep(3, numNodes)
maxParents <- 3
# Function for sufficient stats
functSuffStats <- createSufficientStatsProvider(cardinalities, mObs)
# Local structure score function log(score(Xi, Pa(Xi) | D, <))
functLogLocalStructureScore <- createLogLocalStructureScoringFunction(cardinalities, maxParents, functSuffStats)
# Cache all the local structure scores
system.time(scoreList <- computeFamilyScores(functLogLocalStructureScore, numNodes, maxParents))
# Data and parameters for MCMC
mObs <- as.matrix(training_data)
varNames <- names(training_data)
numNodes <- length(varNames)
cardinalities <- rep(3, numNodes)
maxParents <- 3
# Function for sufficient stats
functSuffStats <- createSufficientStatsProvider(cardinalities, mObs)
# Local structure score function log(score(Xi, Pa(Xi) | D, <))
functLogLocalStructureScore <- createLogLocalStructureScoringFunction(cardinalities, maxParents, functSuffStats)
# Cache all the local structure scores
system.time(scoreList <- computeFamilyScores(functLogLocalStructureScore, numNodes, maxParents))
# Replace the local structure scoring function with its cached version
functLogLocalStructureScore <- function(node, parents, vOrder) {
scoreList$getFamilyScore(node, parents)
}
# Local order score i.e term of node in log P(D | <)
pruningDiff <- 7 # best family consistent with an order exp(7) times more probable than worst included in computations
functLogLocalOrderScore <- function(node, vOrder) {
getLogSumOfExponentials( scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)$scores )
}
# order-MCMC
numSamples <- 5000
system.time(result <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
plot(rowSums(result$logScores), type="l")
load("~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/training_data_and_test_data.RData")
library("gtools")
library("hash")
library("R.utils")
library("rbenchmark")
library("testthat")
sourceDir <- "~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/functions"
sourceDirectory(sourceDir)
# Data and parameters for MCMC
mObs <- as.matrix(training_data)
varNames <- names(training_data)
numNodes <- length(varNames)
cardinalities <- rep(3, numNodes)
maxParents <- 3
# Function for sufficient stats
functSuffStats <- createSufficientStatsProvider(cardinalities, mObs)
# Local structure score function log(score(Xi, Pa(Xi) | D, <))
functLogLocalStructureScore <- createLogLocalStructureScoringFunction(cardinalities, maxParents, functSuffStats)
# Cache all the local structure scores
system.time(scoreList <- computeFamilyScores(functLogLocalStructureScore, numNodes, maxParents))
library("gtools")
library("hash")
library("R.utils")
library("rbenchmark")
library("testthat")
sourceDir <- "~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/functions"
sourceDirectory(sourceDir)
load("~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/training_data_and_test_data.RData")
# Data and parameters for MCMC
mObs <- as.matrix(training_data)
varNames <- names(training_data)
numNodes <- length(varNames)
cardinalities <- rep(3, numNodes)
maxParents <- 3
# Function for sufficient stats
functSuffStats <- createSufficientStatsProvider(cardinalities, mObs)
# Local structure score function log(score(Xi, Pa(Xi) | D, <))
functLogLocalStructureScore <- createLogLocalStructureScoringFunction(cardinalities, maxParents, functSuffStats)
# Cache all the local structure scores
system.time(scoreList <- computeFamilyScores(functLogLocalStructureScore, numNodes, maxParents))
# Replace the local structure scoring function with its cached version
functLogLocalStructureScore <- function(node, parents, vOrder) {
scoreList$getFamilyScore(node, parents)
}
# Local order score i.e term of node in log P(D | <)
pruningDiff <- 7 # best family consistent with an order exp(7) times more probable than worst included in computations
functLogLocalOrderScore <- function(node, vOrder) {
getLogSumOfExponentials( scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)$scores )
}
# order-MCMC
numSamples <- 25000
system.time(result <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
plot(rowSums(result$logScores), type="l")
result$samples[1,]
result$samples[5000:5100,]
result1 <- result
save.image("~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/order_mcmc_training_data_run_1.RData")
load("~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/order_mcmc_training_data_run_2.RData")
sampleIdx <- seq(from=5000, to=numSamples, by=200)
plot(rowSums(result1$logScores[5000:25000,]), type="l")
sampleIdx <- seq(from=5000, to=numSamples, by=200)
sampleIdx <- seq(from=5000, to=numSamples, by=200)
samples1 <- result1$samples[sampleIdx,]
sampleLogScores1 <- result1$logScores[sampleIdx,]
samples2 <- result2$samples[sampleIdx,]
sampleLogScores2 <- result2$logScores[sampleIdx,]
samples <- rbind(samples1, samples2)
sampleLogScores <- rbind(sampleLogScores1, sampleLogScores2)
# Compute edge probabilities
mEdgeProb <- getEdgeProbabilities(samples, maxParents, functLogLocalStructureScore, sampleLogScores)
mTestObs <- as.matrix(test_data)
# compute the predicted test vector probabilities using all the samples
functNodeStateProb <- createStateProbabilityFunction(cardinalities, mObs, functSuffStats=functSuffStats)
functFamiliesAndLogStructureScores <- function(node, vOrder) {
scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)
}
system.time({
vEstimatedObsProbs <- getStateVectorProbability(mTestObs, samples, maxParents, functNodeStateProb, functFamiliesAndLogStructureScores)
})
# normalize vEstTestObsProbs
vEstTestObsProbsNorm <- vEstTestObsProbs/sum(vEstTestObsProbs)
# compute the predicted test vector probabilities using all the samples
functNodeStateProb <- createStateProbabilityFunction(cardinalities, mObs, functSuffStats=functSuffStats)
functFamiliesAndLogStructureScores <- function(node, vOrder) {
scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)
}
system.time({
vEstimatedObsProbs <- getStateVectorProbability(mTestObs, samples, maxParents, functNodeStateProb, functFamiliesAndLogStructureScores)
})
library("gtools")
library("hash")
library("R.utils")
library("rbenchmark")
library("testthat")
sourceDir <- "~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/functions"
sourceDirectory(sourceDir)
library("gtools")
library("hash")
library("R.utils")
library("rbenchmark")
library("testthat")
sourceDir <- "~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/functions"
sourceDirectory(sourceDir)
devel_data <- read.table("~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/devel_data.txt", header=T, quote="\"")
View(devel_data)
devel_probs <- read.table("~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/devel_probs.txt", quote="\"")
View(devel_probs)
# Data and parameters for MCMC
mObs <- as.matrix(devel_data)
varNames <- names(devel_data)
numNodes <- length(varNames)
cardinalities <- rep(3, numNodes)
maxParents <- 3
mTestObs <- as.matrix(test_data)
# Function for sufficient stats
functSuffStats <- createSufficientStatsProvider(cardinalities, mObs)
# Local structure score function log(score(Xi, Pa(Xi) | D, <))
functLogLocalStructureScore <- createLogLocalStructureScoringFunction(cardinalities, maxParents, functSuffStats)
# Cache all the local structure scores
system.time(scoreList <- computeFamilyScores(functLogLocalStructureScore, numNodes, maxParents))
# Data and parameters for MCMC
mObs <- as.matrix(devel_data)
varNames <- names(devel_data)
numNodes <- length(varNames)
cardinalities <- rep(3, numNodes)
maxParents <- 3
mTestObs <- as.matrix(devel_data)
# Function for sufficient stats
functSuffStats <- createSufficientStatsProvider(cardinalities, mObs)
# Local structure score function log(score(Xi, Pa(Xi) | D, <))
functLogLocalStructureScore <- createLogLocalStructureScoringFunction(cardinalities, maxParents, functSuffStats)
# Cache all the local structure scores
system.time(scoreList <- computeFamilyScores(functLogLocalStructureScore, numNodes, maxParents))
# Replace the local structure scoring function with its cached version
functLogLocalStructureScore <- function(node, parents, vOrder) {
scoreList$getFamilyScore(node, parents)
}
# Local order score i.e term of node in log P(D | <)
pruningDiff <- 7 # best family consistent with an order exp(pruningDiff) times more probable than worst included in computations
functLogLocalOrderScore <- function(node, vOrder) {
getLogSumOfExponentials( scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)$scores )
}
# This is needed to compute edge and vector probabilities
functFamiliesAndLogStructureScores <- function(node, vOrder) {
scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)
}
# order-MCMC
numSamples <- 25000
# Chain 1
system.time(result1 <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
plot(rowSums(result1$logScores), type="l", col="red")
# Chain 2
system.time(result2 <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
lines(rowSums(result2$logScores), type="l", col="blue")
# Combine samples of two chains
sampleIdx <- seq(from=5000, to=numSamples, by=200)
samples1 <- result1$samples[sampleIdx,]
sampleLogScores1 <- result1$logScores[sampleIdx,]
samples2 <- result2$samples[sampleIdx,]
sampleLogScores2 <- result2$logScores[sampleIdx,]
samples <- rbind(samples1, samples2)
sampleLogScores <- rbind(sampleLogScores1, sampleLogScores2)
# Compute edge probabilities
system.time(mEdgeProb <- getEdgeProbabilities(samples, functFamiliesAndLogStructureScores))
rownames(mEdgeProb) <- varNames
colnames(mEdgeProb) <- varNames
# List of edge probabilities
sourceNames <- character(numNodes^2 - numNodes)
targetNames <- character(numNodes^2 - numNodes)
edgeProbs <- numeric(numNodes^2 - numNodes)
row <- 1
for (i in 1:numNodes) {
for (j in 1:numNodes) {
if (i != j) {
sourceNames[row] <- varNames[i]
targetNames[row] <- varNames[j]
edgeProbs[row] <- mEdgeProb[i,j]
row <- row+1
}
}
}
edgeRanking <- data.frame(source=sourceNames, target=targetNames, probability=edgeProbs)
edgeRanking <- edgeRanking[order(edgeProbs, sourceNames, targetNames, decreasing=TRUE), ]
rownames(edgeRanking) <- NULL
# compute the predicted test vector probabilities using all the samples
sampleSubset <- samples[sample(1:nrow(samples), 50),] # assuming all orders equally probable!
functNodeStateProb <- createStateProbabilityFunction(cardinalities, mObs, functSuffStats=functSuffStats)
system.time({
vEstimatedObsProbs <- getStateVectorProbability(mObs, sampleSubset, functNodeStateProb, functFamiliesAndLogStructureScores)
})
# normalize vEstTestObsProbs
vEstTestObsProbsNorm <- vEstTestObsProbs/sum(vEstTestObsProbs)
# known probabilities
vKnownProbs <- as.vector(devel_probs)
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbs)
# normalize estimated vector probabilities
vEstimatedObsProbsNorm <- vEstimatedObsProbs/sum(vEstimatedObsProbs)
# known probabilities
vKnownProbs <- as.vector(devel_probs)
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
fix(vEstimatedObsProbsNorm)
View(vKnownProbs)
View(vKnownProbs)
vKnownProbs <- numeric(devel_probs)
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
vKnownProbs <- vector(devel_probs)
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
vKnownProbs <- as.vector(devel_probs, mode="numeric")
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
View(vKnownProbs)
length(vKnownProbs)
View(vKnownProbs)
vKnownProbs <- devel_probs[,1]
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
1200/3600
fix(vEstimatedObsProbsNorm)
write.table(vEstimatedObsProbsNorm, "devel_probs_estimated.txt", row.names=FALSE, col.names=FALSE)
sampleSubset <- samples[sample(1:nrow(samples), 100),] # assuming all orders equally probable!
functNodeStateProb <- createStateProbabilityFunction(cardinalities, mObs, functSuffStats=functSuffStats)
system.time({
vEstimatedObsProbs <- getStateVectorProbability(mObs, sampleSubset, functNodeStateProb, functFamiliesAndLogStructureScores)
})
# normalize estimated vector probabilities
vEstimatedObsProbsNorm <- vEstimatedObsProbs/sum(vEstimatedObsProbs)
# known probabilities
vKnownProbs <- devel_probs[,1]
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
6300/3600
# Local order score i.e term of node in log P(D | <)
pruningDiff <- 8 # best family consistent with an order exp(pruningDiff) times more probable than worst included in computations
functLogLocalOrderScore <- function(node, vOrder) {
getLogSumOfExponentials( scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)$scores )
}
# This is needed to compute edge and vector probabilities
functFamiliesAndLogStructureScores <- function(node, vOrder) {
scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)
}
# order-MCMC
numSamples <- 25000
# Chain 1
system.time(result1 <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
plot(rowSums(result1$logScores), type="l", col="red")
# Chain 2
system.time(result2 <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
lines(rowSums(result2$logScores), type="l", col="blue")
# Combine samples of two chains
sampleIdx <- seq(from=5000, to=numSamples, by=200)
samples1 <- result1$samples[sampleIdx,]
sampleLogScores1 <- result1$logScores[sampleIdx,]
samples2 <- result2$samples[sampleIdx,]
sampleLogScores2 <- result2$logScores[sampleIdx,]
samples <- rbind(samples1, samples2)
sampleLogScores <- rbind(sampleLogScores1, sampleLogScores2)
# Compute edge probabilities
system.time(mEdgeProb <- getEdgeProbabilities(samples, functFamiliesAndLogStructureScores))
rownames(mEdgeProb) <- varNames
colnames(mEdgeProb) <- varNames
# List of edge probabilities
sourceNames <- character(numNodes^2 - numNodes)
targetNames <- character(numNodes^2 - numNodes)
edgeProbs <- numeric(numNodes^2 - numNodes)
row <- 1
for (i in 1:numNodes) {
for (j in 1:numNodes) {
if (i != j) {
sourceNames[row] <- varNames[i]
targetNames[row] <- varNames[j]
edgeProbs[row] <- mEdgeProb[i,j]
row <- row+1
}
}
}
edgeRanking <- data.frame(source=sourceNames, target=targetNames, probability=edgeProbs)
edgeRanking <- edgeRanking[order(edgeProbs, sourceNames, targetNames, decreasing=TRUE), ]
rownames(edgeRanking) <- NULL
# compute the predicted test vector probabilities using all the samples
sampleSubset <- samples[sample(1:nrow(samples), 50),] # assuming all orders equally probable!
functNodeStateProb <- createStateProbabilityFunction(cardinalities, mObs, functSuffStats=functSuffStats)
system.time({
vEstimatedObsProbs <- getStateVectorProbability(mObs, sampleSubset, functNodeStateProb, functFamiliesAndLogStructureScores)
})
# normalize estimated vector probabilities
vEstimatedObsProbsNorm <- vEstimatedObsProbs/sum(vEstimatedObsProbs)
# known probabilities
vKnownProbs <- devel_probs[,1]
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
sampleSubset <- samples[sample(1:nrow(samples), 1),] # assuming all orders equally probable!
functNodeStateProb <- createStateProbabilityFunction(cardinalities, mObs, functSuffStats=functSuffStats)
system.time({
vEstimatedObsProbs <- getStateVectorProbability(mObs, sampleSubset, functNodeStateProb, functFamiliesAndLogStructureScores)
})
# normalize estimated vector probabilities
vEstimatedObsProbsNorm <- vEstimatedObsProbs/sum(vEstimatedObsProbs)
# known probabilities
vKnownProbs <- devel_probs[,1]
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
# Local order score i.e term of node in log P(D | <)
pruningDiff <- 3 # best family consistent with an order exp(pruningDiff) times more probable than worst included in computations
functLogLocalOrderScore <- function(node, vOrder) {
getLogSumOfExponentials( scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)$scores )
}
# This is needed to compute edge and vector probabilities
functFamiliesAndLogStructureScores <- function(node, vOrder) {
scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)
}
# order-MCMC
numSamples <- 25000
# Chain 1
system.time(result1 <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
plot(rowSums(result1$logScores), type="l", col="red")
# Chain 2
system.time(result2 <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
lines(rowSums(result2$logScores), type="l", col="blue")
# Combine samples of two chains
sampleIdx <- seq(from=5000, to=numSamples, by=200)
samples1 <- result1$samples[sampleIdx,]
sampleLogScores1 <- result1$logScores[sampleIdx,]
samples2 <- result2$samples[sampleIdx,]
sampleLogScores2 <- result2$logScores[sampleIdx,]
samples <- rbind(samples1, samples2)
sampleLogScores <- rbind(sampleLogScores1, sampleLogScores2)
# Compute edge probabilities
system.time(mEdgeProb <- getEdgeProbabilities(samples, functFamiliesAndLogStructureScores))
rownames(mEdgeProb) <- varNames
colnames(mEdgeProb) <- varNames
# List of edge probabilities
sourceNames <- character(numNodes^2 - numNodes)
targetNames <- character(numNodes^2 - numNodes)
edgeProbs <- numeric(numNodes^2 - numNodes)
row <- 1
for (i in 1:numNodes) {
for (j in 1:numNodes) {
if (i != j) {
sourceNames[row] <- varNames[i]
targetNames[row] <- varNames[j]
edgeProbs[row] <- mEdgeProb[i,j]
row <- row+1
}
}
}
edgeRanking <- data.frame(source=sourceNames, target=targetNames, probability=edgeProbs)
edgeRanking <- edgeRanking[order(edgeProbs, sourceNames, targetNames, decreasing=TRUE), ]
rownames(edgeRanking) <- NULL
# compute the predicted test vector probabilities using all the samples
sampleSubset <- samples[sample(1:nrow(samples), 1),] # assuming all orders equally probable!
functNodeStateProb <- createStateProbabilityFunction(cardinalities, mObs, functSuffStats=functSuffStats)
system.time({
vEstimatedObsProbs <- getStateVectorProbability(mObs, sampleSubset, functNodeStateProb, functFamiliesAndLogStructureScores)
})
# normalize estimated vector probabilities
vEstimatedObsProbsNorm <- vEstimatedObsProbs/sum(vEstimatedObsProbs)
# known probabilities
vKnownProbs <- devel_probs[,1]
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
# Data and parameters for MCMC
mObs <- as.matrix(devel_data)
varNames <- names(devel_data)
numNodes <- length(varNames)
cardinalities <- rep(3, numNodes)
maxParents <- 3
mTestObs <- as.matrix(devel_data)
# Function for sufficient stats
functSuffStats <- createSufficientStatsProvider(cardinalities, mObs)
# Local structure score function log(score(Xi, Pa(Xi) | D, <))
functLogLocalStructureScore <- createLogLocalStructureScoringFunction(cardinalities, maxParents, functSuffStats)
# Cache all the local structure scores
system.time(scoreList <- computeFamilyScores(functLogLocalStructureScore, numNodes, maxParents))
# Replace the local structure scoring function with its cached version
functLogLocalStructureScore <- function(node, parents, vOrder) {
scoreList$getFamilyScore(node, parents)
}
# Local order score i.e term of node in log P(D | <)
pruningDiff <- 1 # best family consistent with an order exp(pruningDiff) times more probable than worst included in computations
functLogLocalOrderScore <- function(node, vOrder) {
getLogSumOfExponentials( scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)$scores )
}
# This is needed to compute edge and vector probabilities
functFamiliesAndLogStructureScores <- function(node, vOrder) {
scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)
}
# order-MCMC
numSamples <- 25000
# Chain 1
system.time(result1 <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
plot(rowSums(result1$logScores), type="l", col="red")
# Chain 2
system.time(result2 <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
lines(rowSums(result2$logScores), type="l", col="blue")
# Combine samples of two chains
sampleIdx <- seq(from=5000, to=numSamples, by=200)
samples1 <- result1$samples[sampleIdx,]
sampleLogScores1 <- result1$logScores[sampleIdx,]
samples2 <- result2$samples[sampleIdx,]
sampleLogScores2 <- result2$logScores[sampleIdx,]
samples <- rbind(samples1, samples2)
sampleLogScores <- rbind(sampleLogScores1, sampleLogScores2)
# Compute edge probabilities
system.time(mEdgeProb <- getEdgeProbabilities(samples, functFamiliesAndLogStructureScores))
rownames(mEdgeProb) <- varNames
colnames(mEdgeProb) <- varNames
# List of edge probabilities
sourceNames <- character(numNodes^2 - numNodes)
targetNames <- character(numNodes^2 - numNodes)
edgeProbs <- numeric(numNodes^2 - numNodes)
row <- 1
for (i in 1:numNodes) {
for (j in 1:numNodes) {
if (i != j) {
sourceNames[row] <- varNames[i]
targetNames[row] <- varNames[j]
edgeProbs[row] <- mEdgeProb[i,j]
row <- row+1
}
}
}
edgeRanking <- data.frame(source=sourceNames, target=targetNames, probability=edgeProbs)
edgeRanking <- edgeRanking[order(edgeProbs, sourceNames, targetNames, decreasing=TRUE), ]
rownames(edgeRanking) <- NULL
# compute the predicted test vector probabilities using all the samples
sampleSubset <- samples[sample(1:nrow(samples), 1),] # assuming all orders equally probable!
functNodeStateProb <- createStateProbabilityFunction(cardinalities, mObs, functSuffStats=functSuffStats)
system.time({
vEstimatedObsProbs <- getStateVectorProbability(mObs, sampleSubset, functNodeStateProb, functFamiliesAndLogStructureScores)
})
# normalize estimated vector probabilities
vEstimatedObsProbsNorm <- vEstimatedObsProbs/sum(vEstimatedObsProbs)
# known probabilities
vKnownProbs <- devel_probs[,1]
vKnownProbs <- vKnownProbs/sum(vKnownProbs)
getKLDivergence(vKnownProbs, vEstimatedObsProbsNorm)
source('~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/functions/scoring_functions.R')
source('~/Hommia/Santsaus/Project in Probabilistic Models (582637)/my-r-order-mcmc/functions/scoring_functions.R')
# Local structure score function log(score(Xi, Pa(Xi) | D, <))
functLogLocalStructureScore <- createLogLocalStructureScoringFunction(cardinalities, maxParents, functSuffStats)
# Cache all the local structure scores
system.time(scoreList <- computeFamilyScores(functLogLocalStructureScore, numNodes, maxParents))
# Replace the local structure scoring function with its cached version
functLogLocalStructureScore <- function(node, parents, vOrder) {
scoreList$getFamilyScore(node, parents)
}
# Local order score i.e term of node in log P(D | <)
pruningDiff <- Inf # best family consistent with an order exp(pruningDiff) times more probable than worst included in computations
functLogLocalOrderScore <- function(node, vOrder) {
getLogSumOfExponentials( scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)$scores )
}
# This is needed to compute edge and vector probabilities
functFamiliesAndLogStructureScores <- function(node, vOrder) {
scoreList$getFamiliesAndScores(node, vOrder, pruningDiff)
}
# order-MCMC
numSamples <- 25000
# Chain 1
system.time(result1 <- runOrderMCMC(numNodes, maxParents, functLogLocalOrderScore, numSamples))
